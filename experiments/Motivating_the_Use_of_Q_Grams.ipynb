{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "To justify the use of q-grams over traditional word-level tokenization, a two-stage experiment was conducted. In the first stage, a classifier was trained using Bag-of-Words (BoW) features derived from standard word-level tokens. In the second stage, the same experimental setup was applied using q-gram tokenization instead.\n",
        "\n",
        "The results are visualized in a scatter plot, where each point represents the F1-score of a model evaluated on a specific dataset. The x-axis shows performance using word-level tokenization (baseline), while the y-axis shows performance using q-grams. The diagonal line represents equal performance between both methods.\n",
        "\n",
        "Points above the diagonal indicate performance improvement with q-grams, while points below indicate a drop in performance. The overall distribution of points reveals a clear trend favoring q-grams, supporting their effectiveness in handling informal language variations and sparsity in short-text NLP tasks such as tweet classification."
      ],
      "metadata": {
        "id": "V-vnCHG6j1nG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1snk2jTcBCW"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import microtc\n",
        "except ImportError:\n",
        "    !pip install microtc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1TAAxVucA9p"
      },
      "outputs": [],
      "source": [
        "from microtc import TextModel\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import f1_score, recall_score, precision_score\n",
        "from glob import glob\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from microtc.utils import tweet_iterator\n",
        "from scipy.stats import ttest_rel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVjYZimeDD7v"
      },
      "outputs": [],
      "source": [
        "def performance(y, hy):\n",
        "    funcs =  [f1_score, recall_score, precision_score]\n",
        "    B = []\n",
        "    for _ in range(500):\n",
        "        s = np.random.randint(y.shape[0], size=y.shape[0])\n",
        "        # s tiene los índices con reemplazo (repetidos) para usarlos en el arreglos de y_test y hy como re-muestreo para Boostraping\n",
        "        _ = [func(y[s], hy[s]) for func in funcs] # f1_score(y_true, y_pred). aplicar las 3 métricas a y_test y hy re-ordenados por s\n",
        "        B.append(_)\n",
        "    B = np.array(B)\n",
        "    medida = [func(y, hy) for func in funcs]\n",
        "    error_estandar = B.std(axis=0) # desviación de las 3 métricas en los 500 renglones\n",
        "    return medida, error_estandar.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahAzpCpI95Ui"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    text_model_words = TextModel(token_list=[ -1], del_diac=False, num_option='delete', del_punc=False, url_option='delete', del_dup=True, lc=True, hashtag_option=None, q_grams_words=True)\n",
        "    print(text_model_words.tokenize('¿Hola Mundo Cruel villáno?'))\n",
        "\n",
        "    token_list = [-1, 2, 3, 4, (2, 1)]\n",
        "    #token_list = [-1, 2, 3, 4]\n",
        "    text_model_grams = TextModel(token_list=token_list, del_diac=True, num_option='delete', del_punc=True, url_option='delete', del_dup=False, lc=True, hashtag_option=None, q_grams_words=False)\n",
        "    print(text_model_grams.tokenize('Hola Mundo cruel villáno?'))\n",
        "\n",
        "    #linearSVC = LinearSVC(penalty='l1', C=1.0, dual=False, max_iter=10000, random_state = 42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9TwDEHpUDsA"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stopwords.words('spanish')[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EG-rX3LUkGB"
      },
      "outputs": [],
      "source": [
        "stop_words_sp = set(stopwords.words('spanish'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sB4JqCzWBzhY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uexUjgvMO6fi"
      },
      "outputs": [],
      "source": [
        "text_model_words = TextModel(token_list=[-1], del_diac=False, num_option=None, del_punc=False, url_option='delete', del_dup=False, lc=True, hashtag_option=None, q_grams_words=True)\n",
        "\n",
        "# token_list = [2, 3, 4]\n",
        "token_list = [-1, 2, 3, 4]\n",
        "text_model_grams = TextModel(token_list=token_list, del_diac=True, num_option='delete', del_punc=True, url_option='delete', del_dup=True, lc=True, hashtag_option=None, q_grams_words=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzqdW4bGp668"
      },
      "outputs": [],
      "source": [
        "# This cell will likely fail or produce an empty plot as df is not loaded.\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd # Added import for pd\n",
        "try:\n",
        "    # Attempt to create an empty DataFrame if 'df' is not defined\n",
        "    if 'df' not in locals() and 'df' not in globals():\n",
        "        print(\"DataFrame 'df' not defined. Creating an empty DataFrame for plot generation.\")\n",
        "        df = pd.DataFrame(columns=['Dataset', 'F1_d', 'Precision_d', 'Recall_d', 'F1', 'F1_q'])\n",
        "    \n",
        "    dff = df[[\"Dataset\", \"F1_d\", \"Precision_d\", \"Recall_d\"]]\n",
        "    dff[\"Dataset\"] = dff[\"Dataset\"].str.replace(\"grams/\", \"\", regex=False)\n",
        "    dff = dff.sort_values(by='F1_d', ascending=True)\n",
        "    print(dff)\n",
        "except NameError:\n",
        "    print(\"DataFrame 'df' not defined due to removal of data loading cells.\")\n",
        "    dff = pd.DataFrame(columns=[\"Dataset\", \"F1_d\", \"Precision_d\", \"Recall_d\"]) # Create empty df for plotting\n",
        "\n",
        "colors = ['green' if delta >= 0 else 'orange' for delta in dff['F1_d']]\n",
        "\n",
        "dff['Acronym'] = ['' + str(i+1) for i in range(len(dff))]\n",
        "\n",
        "plt.figure(figsize=(6.5, 4.5))\n",
        "plt.barh(dff[\"Dataset\"], dff[\"F1_d\"], color=colors) # Changed from dff.plot to plt.barh for simplicity with potentially empty df\n",
        "\n",
        "plt.xlabel(\"Differences in Metric Scores\")\n",
        "plt.ylabel(\"Dataset\")\n",
        "plt.title(\"Impact of q-gram tokenization in performance metrics\\nacross multiple datasets\", fontsize=11)\n",
        "#plt.legend([\"f1-score\", \"precision\", \"recall\"], title='Metrics', title_fontsize='9') # Legend won't work well with barh like this if F1_d is the only y\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(len(dff))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOYlIfK_C4eP"
      },
      "outputs": [],
      "source": [
        "# This cell will also likely fail as df is not loaded.\n",
        "import pandas as pd # Added import for pd\n",
        "try:\n",
        "    # Attempt to create an empty DataFrame if 'df' is not defined\n",
        "    if 'df' not in locals() and 'df' not in globals():\n",
        "        print(\"DataFrame 'df' not defined. Creating an empty DataFrame.\")\n",
        "        df = pd.DataFrame(columns=['Dataset', 'F1_d', 'Precision_d', 'Recall_d', 'F1', 'F1_q'])\n",
        "    df['Dataset'] = df['Dataset'].str.replace('grams/', '', regex=False)\n",
        "    print(\"Processed df['Dataset']\")\n",
        "except NameError:\n",
        "    print(\"DataFrame 'df' not defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ancqnX2FKGxe"
      },
      "outputs": [],
      "source": [
        "# This cell will also likely fail as df is not loaded.\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd # Added import for pd\n",
        "try:\n",
        "    # Attempt to create an empty DataFrame if 'df' is not defined\n",
        "    if 'df' not in locals() and 'df' not in globals():\n",
        "        print(\"DataFrame 'df' not defined. Creating an empty DataFrame for plot generation.\")\n",
        "        df = pd.DataFrame(columns=['Dataset', 'F1_d', 'Precision_d', 'Recall_d', 'F1', 'F1_q'])\n",
        "\n",
        "    colors = ['green' if delta >= 0 else 'orange' for delta in df['F1_d']]\n",
        "    df['Acronym'] = ['' + str(i+1) for i in range(len(df))]\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    plt.scatter(df.F1, df.F1_q, color=colors)\n",
        "    for i, row in df.iterrows():\n",
        "        plt.text(row['F1'] + 0.002, row['F1_q'], row['Acronym'])\n",
        "    plt.plot([0.5, 0.9], [0.5, 0.9], linestyle='--', color='gray', label='y = x')\n",
        "    plt.text(0.62, 0.61, 'Baseline (y = x)', color='gray', fontsize=11, rotation=34)\n",
        "    plt.xlabel(\"F1-score without q-grams (baseline)\")\n",
        "    plt.ylabel(\"F1-score with q-grams\")\n",
        "    plt.title(\"Effect of q-grams on F1-score across datasets\")\n",
        "    plt.grid(True)\n",
        "    labels = [f\"{row['Acronym']}: {row['Dataset']}\" for _, row in df.iterrows()]\n",
        "    legend_text = \"Datasets:\\n\" + \"\\n\".join(labels)\n",
        "    props = dict(boxstyle='round', facecolor='white', alpha=0.8)\n",
        "    plt.gcf().text(0.70, 0.10, legend_text, fontsize=10, bbox=props)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "except NameError:\n",
        "    print(\"DataFrame 'df' not defined. Plotting will be skipped.\")\n",
        "except KeyError as e:\n",
        "    print(f\"KeyError: {e} - This likely means the DataFrame 'df' is empty or missing expected columns. Plotting will be skipped.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
